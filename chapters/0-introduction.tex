
Welcome to \textit{Development Research in Practice: The DIME Analytics Data Handbook}.
This book is intended to teach all users of development data
how to handle data effectively, efficiently, and ethically.
An empirical revolution has changed the face of development research rapidly over the last decade.
Increasingly, researchers are working not just with complex data,
but with \textit{original} data:
datasets collected by the research team themselves
or acquired through a unique agreement with a project partner.
Research teams must carefully document how original data is created, handled, and analyzed.
These tasks now contribute as much weight to the quality of the evidence
as the research design and the statistical approaches do.
At the same time, the scope and scale of empirical research projects is expanding:
more people are working on the same data over longer timeframes.
For that reason, the central premise of this book is that data work is a ``social process''.
This means that the many different people on a team need to have the same ideas
about what is to be done, and when and where and by whom,
so that they can collaborate effectively on a large, long-term research project.

Despite the growing importance of managing data work,
little practical guidance is available for practitioners.
There are few guides to the conventions, standards, and best practices
that are fast becoming a necessity for empirical research.
\textit{Development Research in Practice} aims to fill that gap.
It covers the full data workflow for a complex research project using original data.
In this book, we share the lessons, tools, and processes
developed based on DIME's diverse research experience,
and compile them into a single narrative of best practices for data work.
This book is not sector-specific;
it will not teach you econometrics,
or how to design an impact evaluation.
There are many excellent existing resources on those topics.
Instead, it will teach you how to think about all aspects of your research from a data perspective,
how to structure research projects to ensure data quality,
and how to institute transparent and reproducible workflows.




%------------------------------------------------

\section{The team behind this book}
DIME is the Development Impact Evaluation department of the World Bank.\footnote{
  \url{https://www.worldbank.org/en/research/dime}}
Its mission is to generate high-quality and operationally relevant data and research
to transform development policy, help reduce extreme poverty, and secure shared prosperity.\footnote{\cite{legovini2015impact}}
DIME develops customized data and evidence ecosystems to produce actionable information
and recommend specific policy pathways to maximize impact.
The department conduct research in 60 countries with 200 agencies, leveraging a
US\$180 million research budget to shape the design and implementation of
US\$18 billion in development finance.
DIME also provides advisory services to 30 multilateral and bilateral development agencies.\footnote{\cite{legovini2019}}
DIME research is organized into four primary topic pillars:
Economic Transformation and Growth;
Gender, Economic Opportunity, and Fragility;
Governance and Institution Building;
and Infrastructure and Climate Change.
Over the years, DIME has employed dozens of research economists,
and hundreds of full-time research assistants, field coordinators, and other staff.
The team has conducted over 325 impact evaluations.
\textit{Development Research in Practice} exists to take advantage of that concentration and scale of research,
to synthesize many resources for data collection and research,
and to make DIME tools available to the larger community of development researchers.

As part of its broader mission, DIME invests in public goods
to improve the quality and reproducibility of development research around the world.
One key early innovation at DIME was the creation of DIME Analytics,
the team responsible for writing and maintaining this book.\footnote{
  \url{https://www.worldbank.org/en/research/dime/data-and-analytics}}
DIME Analytics is a centralized unit that develops and ensures adoption
of high quality research practices across the department's portfolio.
This is done through an intensive, collaborative innovation cycle:
DIME Analytics onboards and supports research assistants and field coordinators,
provides standard tools and workflows to all teams,
delivers hands-on support when new tasks or challenges arise,
and then develops and integrates lessons from those engagements to bring to the full team.
Resources developed and tested in DIME are converted into public goods
for the global research community, through open-access trainings and open-source tools.
The DIME Analytics Resource Directory appendix provides an introduction to public materials.

DIME Analytics has invested many hours over the past years
learning from data work across DIME's portfolio,
identifying inefficiencies and barriers to success,
developing tools and trainings, and standardizing best-practice workflows adopted in DIME projects.
It has also invested significant energy in the language and materials
used to teach these workflows to new team members,
and, in many cases, in software tools that support these workflows explicitly.
DIME team members often work on diverse portfolios of projects
with a wide range of teammates, and we have found
that standardizing core processes across all projects
results in higher-quality work with fewer opportunities to make mistakes.
In that way, the Analytics team is DIME's method of institutionalizing
tools and practices, developed and refined over time,
that give the department a common base of knowledge and practice.
In 2018, for example, DIME adopted universal reproducibility checks
conducted by the Analytics team;
the lessons from this practice helped move the DIME team
from where 50\% of submitted papers in 2018
required significant revision to pass
to where 64\% of papers passed in 2019 without revision required.


%------------------------------------------------


	\begin{figure}
		\centering
		\includegraphics[width=1.5\linewidth]{diagrams/Introduction}
		\label{fig:intro}
	\end{figure}


\section{How to read this book}
This book aims to be a highly practical resource so the reader can
immediately begin to collaborate more effectively
on large, long-term research projects
that use the methods and tools discussed.
This introduction outlines the basic philosophies
that motivate this book and our approach to research data.
We want all readers to understand at the outset our mindset
that research data work is primarily about
communicating effectively within a team
and that standardization and simplification of data tasks
is a major enabler of effective collaboration.
The main chapters of this book will walk you through the data workflow at each stage
of an empirical research project, from design to publication.
The figure in this introduction visualizes the data workflow.
Chapters 1 and 2 contextualize the workflow,
and set the stage for the hands-on data tasks
which are described in detail in Chapters 3 to 7.

\textbf{Chapter 1} outlines a set of practices and ideals to ensure that
research consumers can be confident in the conclusions reached,
and that the reliability of research work can be verified.
It begins with ethical principles to guide empirical research,
focusing on research reproducibility, transparency, and credibility.
It then introduces tools to document
the aims and methods of a research project,
ensuring that meta-information about your research is available
and that you approach data work with an eye towards the future.

\textbf{Chapter 2} teaches you to structure your data work for collaborative research,
while ensuring the privacy and security of research participants.
It discusses the importance of planning datawork and associated tools in advance,
long before any data is acquired.
It also describes ethical concerns common to development data,
common pitfalls in legal and practical management of data,
and how to respect the rights of research participants at all stages of data work.

\textbf{Chapter 3} turns to the measurement framework,
and how to translate research design to a data work plan.
It details DIME's data map template, a set of tools to communicate the project's data requirements
both across the team and across time.
It also discusses how to implement random sampling and random assignment
in a reproducible and credible manner.

\textbf{Chapter 4} covers data acquisition. It starts with
the legal and institutional frameworks for data ownership and licensing,
to ensure that you are aware of the rights and responsibilities
of using data collected by the research team or by others.
It provides a deep dive on collecting high-quality primary electronic survey data,
including developing and deploying survey instruments.
Finally, it discusses secure data handling during transfer, sharing, and storage,
which is essential in protecting the privacy of respondents in any data.

\textbf{Chapter 5} describes data processing tasks.
It details how to construct ``tidy'' data at the appropriate units of analysis,
how to ensure uniquely identified datasets, and
how to routinely incorporate data quality checks into the workflow.
It also provides guidance on de-identification and cleaning of personally-identified data,
focusing on how to understand and structure data
so that it is ready for indicator construction and analytical work.

\textbf{Chapter 6} discusses data analysis tasks.
It begins with data construction, or the creation of new variables
from the raw data acquired or collected in the field.
It introduces core principles for writing analytical code
and creating, exporting, and storing research outputs
such as figures and tables reproducibly using dynamic documents.

\textbf{Chapter 7} outlines the publication of research outputs,
including manuscripts, code, and data.
This chapter discusses
how to effectively collaborate on technical writing
using dynamic documents.
It also covers how and why to publish datasets
in an accessible, citable, and safe fashion.
Finally, it provides guidelines for preparing
functional and informative reproducibility packages
that contain all the code, data, and meta-information needed
for others to evaluate and reproduce your work.

After reading each chapter, you should understand
what tasks will be performed at every stage of the workflow,
and how to implement them according to best practices.
You should also understand how the various stages of the workflow tie together,
and what inputs and outputs are required and produced from each.
The references and links contained in each chapter
will lead you to detailed descriptions of individual
ideas, tools, and processes to refer to when you need to implement the tasks yourself.
Specific implementation details and links to further practical resources
will often be found on the linked pages in the \textbf{DIME Wiki}.\footnote{
The DIME Wiki is a free online collection of impact evaluation resources and best practices. \url{https://dimewiki.worldbank.org}}.

%------------------------------------------------
\section{Handling original data is a core research task}

In the past, data work was often treated as a ``black box'' in research.
A published manuscript might exhaustively detail
research designs, estimation strategies, and theoretical frameworks,
but typically reserved very little space for detailed descriptions
of how data was actually collected and handled.
It is almost impossible to assess the quality of the data in such a paper,
and whether the results could be reproduced.
In the past decade, this has started to change,\footnote{\cite{swanson2020research}}
in part due to increasing requirements by publishers and funders to release code and data.
Data handling and documentation is a key skill for researchers and research staff.
Standard processes and documentation practices
are important throughout the research process to accurately convey
and implement the intended research design.\footnote{\cite{vilhuber_lars_2020_3911311}}
When data work is done in an ad-hoc manner,
it is very difficult for others to understand what is being done --
a reader has to simply trust that the researchers did these things right.
Most importantly, if any part of the data pipeline breaks down,
research results become unreliable\footnote{ \cite{mccullough2008economics}}
and cannot be faithfully interpreted
as being an accurate picture of the intended research design.\footnote{
  \url{https://blogs.worldbank.org/impactevaluations/more-replication-economics}}
Because we almost never have ``laboratory'' settings\footnote{
  See \cite{baldwin2015elections} for an example.}
in this type of research,
such a failure has a very high cost:
we will have wasted the investments that were made into knowledge generation,
and the research opportunity itself,
where we intended to conduct the study.\footnote{\cite{camerer2016evaluating}}

Accurate and reproducible data management and analysis
is essential to the success and credibility of modern research.
Standardizing and documenting data handling processes is essential
to be able to evaluate and understand
the data work alongside any final research outputs.
An important component of this is \textbf{process standardization}.\footnote{
	\textbf{Process standardization:} Agreement within a research team
	about how all tasks of a specific type will be approached.}\index{process standardization}
Process standardization means that there is
little ambiguity about how something ought to be done,
and therefore the tools to do it can be set in advance.
Standard processes help other people understand your work,
and they also make your work easier to document.
Process standardization and documentation should allow readers of your code to:
(1) quickly understand what a particular process or output is supposed to be doing;
(2) evaluate whether or not it does that thing correctly; and
(3) modify it either to test alternative hypotheses or to adapt into their own work.
This book will discuss specific standards recommended by DIME Analytics,
but we are more interested in convincing the reader
to discuss the adoption of \textit{a} standard within research teams
than to necessarily use \textit{the} particular standards that we recommend.


\section{Documenting data work with standardized code}

Modern quantitative research relies heavily
on standardized statistical software tools,
written with various coding languages, to standardize analytical work.
Outputs like regression tables and data visualizations
are created using code in statistical software for two primary reasons.
The first is that using a standard command or package ensures that the work is done right,
and the second is that it ensures the same procedure can be confirmed or checked
at a later date or using different data.
Keeping a clear, human-readable record of these code and data structures is critical.
While it is often \textit{possible} to perform nearly all the relevant tasks
through an interactive user interface or even through software such as Excel,
this practice is strongly advised against.
In the context of statistical analysis,
the practice of writing all work using standard code is widely accepted.
To support this practice, DIME now maintains portfolio-wide standards
about how analytical code should be maintained and made accessible
before, during, and after release or publication.

Over the last few years, DIME has extended the same principles to preparing data for analysis,
which often comprises just as much (or more) of the manipulation done to the data
over the life cycle of a research project.
A major aim of this book is to encourage research teams
to think of the tools and processes they use
for designing, collecting, and handling data
just as they do for analytical tasks.
Correspondingly, a major contribution of DIME Analytics
has been tools and standard practices
for implementing these tasks using statistical software.

While we assume that you are going to do nearly all data work using code,
many development researchers come from economics and statistics backgrounds
and often understand code to be a means to an end rather than an output itself.
We believe that this must change somewhat:
in particular, we think that development practitioners
must think about their code and programming workflows
just as methodologically as they think about their research workflows,
and think of code and data as research outputs, just as manuscripts and briefs are.

This approach arises because we see the code as the ``recipe'' for the analysis.
The code tells others exactly what was done,
how they can do it again in the future,
and provides a roadmap and knowledge base for further original work.\footnote{\cite{hamermesh2007replication}}
Performing every task through written code
creates a record of every task you performed.\footnote{\cite{ozier2019replication}}
It also prevents direct interaction
with the data files that could lead to non-reproducible processes.\footnote{\cite{chang2015economics}}
Finally, DIME Analytics has invested a lot of time in developing code as a learning tool:
the examples we have written and the commands we provide
are designed to provide a framework for common practice
across the entire DIME team, so that everyone is able to
read, review, and provide feedback on the work of others
starting from the same basic ideas about how various tasks are done.

Most specific code tools have a learning and adaptation process,
meaning you will become most comfortable with each tool
only by using it in real-world work.
To support your process of learning reproducible tools and workflows,
we reference free and open-source tools wherever possible,
and point to more detailed instructions when relevant.
\textbf{Stata},\footnote{\cite{statacorp2019stata}}
as a proprietary software, is the notable exception here
due to its persistent popularity in development economics and econometrics.\footnote{
	\url{https://dimewiki.worldbank.org/Software_Tools}}
This book also includes, in an appendix,
the \textbf{DIME Analytics Coding Guide}
which includes instructions for how to write good code,
instructions on how to use the code examples in this book,
as well as our Stata Style Guide.
DIME projects are strongly encouraged to
explicitly adopt and follow coding style guides in their work.
Style guides harmonize code within and across teams
making it easier to understand and reuse code,
which ultimately helps teams to
build on each other's best practices.
Some of the programming languages used at DIME
already have well-established and commonly used style guides,
such as the Tidyverse style guide for R
and PEP-8 for Python.\footnote{See DIME Analytics Coding Standards:
	\url{https://github.com/worldbank/dime-standards}}.
Stata has relatively few resources of this type available,
which is why we have created and included one here that
we hope will be an asset to all Stata users.

\section{Looking forward}
While adopting the workflows and mindsets described in this book requires an up-front cost,
it will save you (and your collaborators) a lot of time and hassle very quickly.
In part this is because you will learn how to implement essential practices directly;
in part because you will find tools for the more advanced practices;
and most importantly because you will acquire the mindset of doing research with a high-quality data focus.
We hope you will find this book helpful for accomplishing all of the above,
and that mastery of data helps you make an impact.
We hope that by the end of the book,
you will have learned how to handle data more efficiently, effectively and ethically
at all stages of the research process.

\mainmatter
